import streamlit as st
import asyncio
import edge_tts
import io
import re
from pydub import AudioSegment
from datetime import datetime

# --- á€áŸ†áááŸ‹á‘áŸ†á–áŸáš ---
st.set_page_config(page_title="Khmer TTS Pro", page_icon="ğŸ™ï¸")

# á˜á»áá„á¶ášá”áŸ†á”áŸ’á›áŸ‚á„á–áŸá›áœáŸá›á¶á–á¸ SRT á‘áŸ…á‡á¶ ms
def srt_time_to_ms(time_str):
    time_obj = datetime.strptime(time_str.strip().replace(',', '.'), '%H:%M:%S.%f')
    return (time_obj.hour * 3600000) + (time_obj.minute * 60000) + (time_obj.second * 1000) + (time_obj.microsecond // 1000)

# á˜á»áá„á¶ášá‘á¶á‰á™á€á¢ááŸ’áá”á‘á–á¸ SRT
def parse_srt(srt_text):
    pattern = r"(\d+)\s+(\d{2}:\d{2}:\d{2},\d{3}) --> (\d{2}:\d{2}:\d{2},\d{3})\s+(.*?)(?=\n\d+\s+|\Z)"
    matches = re.findall(pattern, srt_text, re.DOTALL)
    subtitles = []
    for m in matches:
        subtitles.append({
            "start": srt_time_to_ms(m[1]),
            "text": m[3].strip()
        })
    return subtitles

# --- á˜á»áá„á¶ášá”á„áŸ’á€á¾ááŸáŸ†á¡áŸá„ ---
async def generate_srt_audio(subtitles, voice, rate, pitch):
    combined_audio = AudioSegment.silent(duration=0)
    current_time = 0

    # á”á“áŸ’ááŸ‚á˜ Rate á“á·á„ Pitch á‘áŸ…á€áŸ’á“á»á„á‘á˜áŸ’ášá„áŸ‹ edge-tts (+10%, -5%, etc.)
    rate_str = f"{rate:+d}%"
    pitch_str = f"{pitch:+d}Hz"

    progress_bar = st.progress(0)
    for i, sub in enumerate(subtitles):
        # á”á„áŸ’á€á¾ááŸáŸ†á¡áŸá„áŠáŸ„á™á˜á¶á“á€áŸ†áááŸ‹á›áŸ’á”á¿á“
        communicate = edge_tts.Communicate(sub["text"], voice, rate=rate_str, pitch=pitch_str)
        temp_buffer = io.BytesIO()
        async for chunk in communicate.stream():
            if chunk["type"] == "audio":
                temp_buffer.write(chunk["data"])
        
        temp_buffer.seek(0)
        segment = AudioSegment.from_file(temp_buffer, format="mp3")

        # á‚áá“á¶á…á“áŸ’á›áŸ„áŸ‡áŸáŸ’á„á¶ááŸ‹
        wait_time = sub["start"] - current_time
        if wait_time > 0:
            combined_audio += AudioSegment.silent(duration=wait_time)
        
        combined_audio += segment
        current_time = sub["start"] + len(segment)
        progress_bar.progress((i + 1) / len(subtitles))

    out_buffer = io.BytesIO()
    combined_audio.export(out_buffer, format="mp3")
    return out_buffer.getvalue()

# --- UI Layout ---
st.title("ğŸ™ï¸ á€á˜áŸ’á˜áœá·á’á¸á¢á¶á“ SRT á€á˜áŸ’ášá·áááŸ’á–áŸáŸ‹")

with st.sidebar:
    st.header("âš™ï¸ á€á¶ášá€áŸ†áááŸ‹áŸáŸ†á¡áŸá„")
    voice_choice = st.selectbox("á‡áŸ’ášá¾áŸášá¾áŸáŸáŸ†á¡áŸá„:", ["áŸáŸ’ášá¸á˜á»áŸ† (Sreymom)", "á–á·áŸá·áŠáŸ’á‹ (Piseth)"])
    voice_id = "km-KH-SreymomNeural" if "áŸáŸ’ášá¸á˜á»áŸ†" in voice_choice else "km-KH-PisethNeural"
    
    # á”á“áŸ’ááŸ‚á˜ Slider áŸá˜áŸ’ášá¶á”áŸ‹á›áŸ’á”á¿á“
    speed_rate = st.slider("á›áŸ’á”á¿á“á¢á¶á“ (%)", min_value=-50, max_value=50, value=0, step=5)
    pitch_val = st.slider("á€á˜áŸ’ášá·ááŸáŸ†á¡áŸá„ Pitch (Hz)", min_value=-20, max_value=20, value=0, step=1)
    
    st.info("ğŸ’¡ á›áŸ’á”á¿á“ (+) á›á¿á“, (-) á™áºá")

srt_input = st.text_area("á”á·á‘á—áŸ’á‡á¶á”áŸ‹á¢ááŸ’áá”á‘ SRT á“áŸ…á‘á¸á“áŸáŸ‡:", height=250, placeholder="1\n00:00:01,000 --> 00:00:02,000\náŸá½áŸáŸ’áá¸...")

if st.button("ğŸš€ á…á¶á”áŸ‹á•áŸ’áá¾á˜á•á›á·ááŸáŸ†á¡áŸá„"):
    if srt_input.strip():
        try:
            subs = parse_srt(srt_input)
            if subs:
                with st.spinner("á€áŸ†á–á»á„á”áŸ†á”áŸ’á›áŸ‚á„..."):
                    audio_data = asyncio.run(generate_srt_audio(subs, voice_id, speed_rate, pitch_val))
                    st.success("áŸá˜áŸ’ášáŸá…! á¢áŸ’á“á€á¢á¶á…áŸáŸ’áá¶á”áŸ‹ á“á·á„á‘á¶á‰á™á€áá¶á„á€áŸ’ášáŸ„á˜áŸ–")
                    st.audio(audio_data, format="audio/mp3")
                    st.download_button("ğŸ“¥ á‘á¶á‰á™á€ MP3", audio_data, "khmer_subtitle_audio.mp3")
        except Exception as e:
            st.error(f"á€áŸ†á á»áŸá”á…áŸ’á…áŸá€á‘áŸáŸáŸ– {e}")
    else:
        st.warning("áŸá¼á˜á”á‰áŸ’á…á¼á›á¢ááŸ’áá”á‘á‡á¶á˜á»á“áŸá·á“!")
